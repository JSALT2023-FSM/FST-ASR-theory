@inproceedings{laptev_ctc_2022,
	title = {{CTC} {Variations} {Through} {New} {WFST} {Topologies}},
	url = {http://arxiv.org/abs/2110.03098},
	doi = {10.21437/interspeech.2022-10854},
	abstract = {This paper presents novel Weighted Finite-State Transducer (WFST) topologies to implement Connectionist Temporal Classification (CTC)-like algorithms for automatic speech recognition. Three new CTC variants are proposed: (1) the "compact-CTC", in which direct transitions between units are replaced with {\textless}epsilon{\textgreater} back-off transitions; (2) the "minimal-CTC", that only adds {\textless}blank{\textgreater} self-loops when used in WFST-composition; and (3) the "selfless-CTC" variants, which disallows self-loop for non-blank units. Compact-CTC allows for 1.5 times smaller WFST decoding graphs and reduces memory consumption by two times when training CTC models with the LF-MMI objective without hurting the recognition accuracy. Minimal-CTC reduces graph size and memory consumption by two and four times for the cost of a small accuracy drop. Using selfless-CTC can improve the accuracy for wide context window models.},
	urldate = {2023-01-22},
	booktitle = {Interspeech 2022},
	author = {Laptev, Aleksandr and Majumdar, Somshubra and Ginsburg, Boris},
	month = sep,
	year = {2022},
	note = {arXiv:2110.03098 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {1041--1045},
	file = {arXiv.org Snapshot:/Users/desh/Zotero/storage/J63PYE9X/2110.html:text/html;Laptev et al_2022_CTC Variations Through New WFST Topologies.pdf:/Users/desh/Zotero/storage/MIKBTWFL/Laptev et al_2022_CTC Variations Through New WFST Topologies.pdf:application/pdf},
}

@misc{miao_eesen_2015,
	title = {{EESEN}: {End}-to-{End} {Speech} {Recognition} using {Deep} {RNN} {Models} and {WFST}-based {Decoding}},
	shorttitle = {{EESEN}},
	url = {http://arxiv.org/abs/1507.08240},
	doi = {10.48550/arXiv.1507.08240},
	abstract = {The performance of automatic speech recognition (ASR) has improved tremendously due to the application of deep neural networks (DNNs). Despite this progress, building a new ASR system remains a challenging task, requiring various resources, multiple training stages and significant expertise. This paper presents our Eesen framework which drastically simplifies the existing pipeline to build state-of-the-art ASR systems. Acoustic modeling in Eesen involves learning a single recurrent neural network (RNN) predicting context-independent targets (phonemes or characters). To remove the need for pre-generated frame labels, we adopt the connectionist temporal classification (CTC) objective function to infer the alignments between speech and label sequences. A distinctive feature of Eesen is a generalized decoding approach based on weighted finite-state transducers (WFSTs), which enables the efficient incorporation of lexicons and language models into CTC decoding. Experiments show that compared with the standard hybrid DNN systems, Eesen achieves comparable word error rates (WERs), while at the same time speeding up decoding significantly.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Miao, Yajie and Gowayyed, Mohammad and Metze, Florian},
	month = oct,
	year = {2015},
	note = {arXiv:1507.08240 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/desh/Zotero/storage/7N5KKPJY/1507.html:text/html;Miao et al_2015_EESEN.pdf:/Users/desh/Zotero/storage/AA5P7CLV/Miao et al_2015_EESEN.pdf:application/pdf},
}

@inproceedings{xiang_crf-based_2019,
	address = {Brighton, United Kingdom},
	title = {{CRF}-based {Single}-stage {Acoustic} {Modeling} with {CTC} {Topology}},
	isbn = {978-1-4799-8131-1},
	url = {https://ieeexplore.ieee.org/document/8682256/},
	doi = {10.1109/ICASSP.2019.8682256},
	abstract = {In this paper, we develop conditional random ﬁeld (CRF) based single-stage (SS) acoustic modeling with connectionist temporal classiﬁcation (CTC) inspired state topology, which is called CTCCRF for short. CTC-CRF is conceptually simple, which basically implements a CRF layer on top of features generated by the bottom neural network with the special state topology. Like SS-LFMMI (lattice-free maximum-mutual-information), CTC-CRFs can be trained from scratch (ﬂat-start), eliminating GMM-HMM pretraining and tree-building. Evaluation experiments are conducted on the WSJ, Switchboard and Librispeech datasets. In a head-tohead comparison, the CTC-CRF model using simple Bidirectional LSTMs consistently outperforms the strong SS-LF-MMI, across all the three benchmarking datasets and in both cases of mono-phones and mono-chars. Additionally, CTC-CRFs avoid some ad-hoc operation in SS-LF-MMI.},
	language = {en},
	urldate = {2023-01-22},
	booktitle = {{ICASSP} 2019 - 2019 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	publisher = {IEEE},
	author = {Xiang, Hongyu and Ou, Zhijian},
	month = may,
	year = {2019},
	pages = {5676--5680},
	file = {Xiang and Ou - 2019 - CRF-based Single-stage Acoustic Modeling with CTC .pdf:/Users/desh/Zotero/storage/5293CRHI/Xiang and Ou - 2019 - CRF-based Single-stage Acoustic Modeling with CTC .pdf:application/pdf},
}

@misc{zhao_novel_2019,
	title = {A {Novel} {Topology} for {End}-to-end {Temporal} {Classification} and {Segmentation} with {Recurrent} {Neural} {Network}},
	url = {http://arxiv.org/abs/1912.04784},
	doi = {10.48550/arXiv.1912.04784},
	abstract = {Connectionist temporal classification (CTC) has matured as an alignment free to sequence transduction and shows competitive for end-to-end speech recognition. In the CTC topology, the blank symbol occupies more than half of the state trellis, which results the spike phenomenon of the non-blank symbols. For classification task, the spikes work quite well, but as to the segmentation task it does not provide boundaries information. In this paper, a novel topology is introduced to combine the temporal classification and segmentation ability in one framework.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Zhao, Taiyang},
	month = dec,
	year = {2019},
	note = {arXiv:1912.04784 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Computation and Language, Computer Science - Sound, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/desh/Zotero/storage/K7JNH6NX/1912.html:text/html;Zhao_2019_A Novel Topology for End-to-end Temporal Classification and Segmentation with.pdf:/Users/desh/Zotero/storage/KJJNAHGK/Zhao_2019_A Novel Topology for End-to-end Temporal Classification and Segmentation with.pdf:application/pdf},
}

@inproceedings{hadian_end--end_2018,
	title = {End-to-end {Speech} {Recognition} {Using} {Lattice}-free {MMI}},
	url = {https://www.isca-speech.org/archive/interspeech_2018/hadian18_interspeech.html},
	doi = {10.21437/Interspeech.2018-1423},
	abstract = {We present our work on end-to-end training of acoustic models using the lattice-free maximum mutual information (LF-MMI) objective function in the context of hidden Markov models. By end-to-end training, we mean ﬂat-start training of a single DNN in one stage without using any previously trained models, forced alignments, or building state-tying decision trees. We use full biphones to enable context-dependent modeling without trees, and show that our end-to-end LF-MMI approach can achieve comparable results to regular LF-MMI on well-known large vocabulary tasks. We also compare with other end-to-end methods such as CTC in character-based and lexicon-free settings and show 5 to 25 percent relative reduction in word error rates on different large vocabulary tasks while using signiﬁcantly smaller models.},
	language = {en},
	urldate = {2023-01-22},
	booktitle = {Interspeech 2018},
	publisher = {ISCA},
	author = {Hadian, Hossein and Sameti, Hossein and Povey, Daniel and Khudanpur, Sanjeev},
	month = sep,
	year = {2018},
	pages = {12--16},
	file = {Hadian et al. - 2018 - End-to-end Speech Recognition Using Lattice-free M.pdf:/Users/desh/Zotero/storage/W9KT66BJ/Hadian et al. - 2018 - End-to-end Speech Recognition Using Lattice-free M.pdf:application/pdf},
}

@misc{zhang_lattice-free_2021,
	title = {On lattice-free boosted {MMI} training of {HMM} and {CTC}-based full-context {ASR} models},
	url = {http://arxiv.org/abs/2107.04154},
	abstract = {Hybrid automatic speech recognition (ASR) models are typically sequentially trained with CTC or LF-MMI criteria. However, they have vastly different legacies and are usually implemented in different frameworks. In this paper, by decoupling the concepts of modeling units and label topologies and building proper numerator/denominator graphs accordingly, we establish a generalized framework for hybrid acoustic modeling (AM). In this framework, we show that LF-MMI is a powerful training criterion applicable to both limited-context and full-context models, for wordpiece/mono-char/bi-char/chenone units, with both HMM/CTC topologies. From this framework, we propose three novel training schemes: chenone(ch)/wordpiece(wp)-CTC-bMMI, and wordpiece(wp)-HMM-bMMI with different advantages in training performance, decoding efficiency and decoding time-stamp accuracy. The advantages of different training schemes are evaluated comprehensively on Librispeech, and wp-CTC-bMMI and ch-CTC-bMMI are evaluated on two real world ASR tasks to show their effectiveness. Besides, we also show bi-char(bc) HMM-MMI models can serve as better alignment models than traditional non-neural GMM-HMMs.},
	urldate = {2023-01-22},
	publisher = {arXiv},
	author = {Zhang, Xiaohui and Manohar, Vimal and Zhang, David and Zhang, Frank and Shi, Yangyang and Singhal, Nayan and Chan, Julian and Peng, Fuchun and Saraf, Yatharth and Seltzer, Mike},
	month = sep,
	year = {2021},
	note = {arXiv:2107.04154 [cs, eess]},
	keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/desh/Zotero/storage/2MKKLFLH/2107.html:text/html;Zhang et al_2021_On lattice-free boosted MMI training of HMM and CTC-based full-context ASR.pdf:/Users/desh/Zotero/storage/HWSURIZ8/Zhang et al_2021_On lattice-free boosted MMI training of HMM and CTC-based full-context ASR.pdf:application/pdf},
}

@misc{laptev_powerful_2023,
	title = {Powerful and {Extensible} {WFST} {Framework} for {RNN}-{Transducer} {Losses}},
	url = {http://arxiv.org/abs/2303.10384},
	abstract = {This paper presents a framework based on Weighted Finite-State Transducers (WFST) to simplify the development of modifications for RNN-Transducer (RNN-T) loss. Existing implementations of RNN-T use CUDA-related code, which is hard to extend and debug. WFSTs are easy to construct and extend, and allow debugging through visualization. We introduce two WFST-powered RNN-T implementations: (1) "Compose-Transducer", based on a composition of the WFST graphs from acoustic and textual schema -- computationally competitive and easy to modify; (2) "Grid-Transducer", which constructs the lattice directly for further computations -- most compact, and computationally efficient. We illustrate the ease of extensibility through introduction of a new W-Transducer loss -- the adaptation of the Connectionist Temporal Classification with Wild Cards. W-Transducer (W-RNNT) consistently outperforms the standard RNN-T in a weakly-supervised data setup with missing parts of transcriptions at the beginning and end of utterances. All RNN-T losses are implemented with the k2 framework and are available in the NeMo toolkit.},
	urldate = {2023-05-06},
	publisher = {arXiv},
	author = {Laptev, Aleksandr and Bataev, Vladimir and Gitman, Igor and Ginsburg, Boris},
	month = mar,
	year = {2023},
	note = {arXiv:2303.10384 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv.org Snapshot:/Users/desh/Zotero/storage/6DH88HUE/2303.html:text/html;Laptev et al_2023_Powerful and Extensible WFST Framework for RNN-Transducer Losses.pdf:/Users/desh/Zotero/storage/6MLYF2X6/Laptev et al_2023_Powerful and Extensible WFST Framework for RNN-Transducer Losses.pdf:application/pdf},
}

@misc{ondel_gpu-accelerated_2021,
	title = {{GPU}-{Accelerated} {Forward}-{Backward} algorithm with {Application} to {Lattice}-{Free} {MMI}},
	url = {http://arxiv.org/abs/2112.00709},
	abstract = {We propose to express the forward-backward algorithm in terms of operations between sparse matrices in a specific semiring. This new perspective naturally leads to a GPU-friendly algorithm which is easy to implement in Julia or any programming languages with native support of semiring algebra. We use this new implementation to train a TDNN with the LF-MMI objective function and we compare the training time of our system with PyChain - a recently introduced C++/CUDA implementation of the LF-MMI loss. Our implementation is about two times faster while not having to use any approximation such as the "leaky-HMM".},
	urldate = {2023-05-06},
	publisher = {arXiv},
	author = {Ondel, Lucas and Lam-Yee-Mui, Léa-Marie and Kocour, Martin and Corro, Caio Filippo and Burget, Lukáš},
	month = oct,
	year = {2021},
	note = {arXiv:2112.00709 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/Users/desh/Zotero/storage/JJGIAIGP/2112.html:text/html;Ondel et al_2021_GPU-Accelerated Forward-Backward algorithm with Application to Lattice-Free MMI.pdf:/Users/desh/Zotero/storage/R8IM9USB/Ondel et al_2021_GPU-Accelerated Forward-Backward algorithm with Application to Lattice-Free MMI.pdf:application/pdf},
}

@article{Mohri2002WeightedFT,
  title={Weighted finite-state transducers in speech recognition},
  author={Mehryar Mohri and Fernando C Pereira and Michael Riley},
  journal={Comput. Speech Lang.},
  year={2002},
  volume={16},
  pages={69-88}
}

@Inbook{Golan1999_1,
author="Golan, Jonathan S.",
title="Hemirings and Semirings: Definitions and Examples",
bookTitle="Semirings and their Applications",
year="1999",
publisher="Springer Netherlands",
address="Dordrecht",
pages="1--18",
abstract="A semigroup (M, *) consists of a nonempty set M on which an associative operation * is defined. If M is a semigroup in which there exists an element e satisfying m * e = m = e * m for all m ∈ M, then M is called a monoid having identity element e. This element can easily seen to be unique, and is usually denoted by 1m- Note that a semigroup (M, *) which is not a monoid can be canonically embedded in a monoid M{\textasciiacutex} - M ∪ {\{}e{\}} where e is some element not in M, and where the operation * is extended to an operation on M{\textasciiacutex} by defining e * M{\textasciiacutex} = M{\textasciiacutex} = M{\textasciiacutex} * e for all m{\textasciiacutex} ∈ m{\textasciiacutex}. An element m of M idempotent if and only if m * m = m. A semigroup (M, *) is commutative if and only if m * M{\textasciiacutex} = M{\textasciiacutex} * m for all m.m{\textasciiacutex} ∈ M.",
isbn="978-94-015-9333-5",
doi="10.1007/978-94-015-9333-5_1",
url="https://doi.org/10.1007/978-94-015-9333-5_1"
}

@article{Dong2020ACO,
  title={A Comparison of Label-Synchronous and Frame-Synchronous End-to-End Models for Speech Recognition},
  author={Linhao Dong and Cheng Yi and Jianzong Wang and Shiyu Zhou and Shuang Xu and Xueli Jia and Bo Xu},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.10113}
}

@inproceedings{Prabhavalkar2017ACO,
  title={A Comparison of Sequence-to-Sequence Models for Speech Recognition},
  author={Rohit Prabhavalkar and Kanishka Rao and Tara N. Sainath and Bo Li and Leif M. Johnson and Navdeep Jaitly},
  booktitle={Interspeech},
  year={2017}
}

@article{Variani2022GlobalNF,
  title={Global Normalization for Streaming Speech Recognition in a Modular Framework},
  author={Ehsan Variani and Ke Wu and Michael Riley and David Rybach and Matt Shannon and Cyril Allauzen},
  journal={ArXiv},
  year={2022},
  volume={abs/2205.13674}
}

@article{Graves2006ConnectionistTC,
  title={Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks},
  author={Alex Graves and Santiago Fern{\'a}ndez and Faustino J. Gomez and J{\"u}rgen Schmidhuber},
  journal={Proceedings of the 23rd international conference on Machine learning},
  year={2006}
}

@inproceedings{Povey2016PurelySN,
  title={Purely Sequence-Trained Neural Networks for ASR Based on Lattice-Free MMI},
  author={Daniel Povey and Vijayaditya Peddinti and Daniel Galvez and Pegah Ghahremani and Vimal Manohar and Xingyu Na and Yiming Wang and Sanjeev Khudanpur},
  booktitle={Interspeech},
  year={2016}
}