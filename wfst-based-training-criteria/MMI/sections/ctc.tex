\section{WFST formulation of CTC}
\label{sec:ctc}
Let us quickly see how the popular CTC model can be formulated as a special case of the general MMI training. For this, we take equation~(\ref{eq:loss}) and specialize it as follows:

\begin{itemize}
    \item $\mathcal{B}$ is set as the mapping generated by the CTC topology.
    \item $p(\mathbf{a})$ is set as the uniform distribution.
    \item $f_{\theta}(\mathbf{X},\mathbf{a}) = \sum_t \phi_{a_t}^t$, i.e., the score of an alignment $\mathbf{a}$ is set as the sum of its path weights in the sausage lattice $\phi$.
\end{itemize}

With these assignments, we can simplify equation~(\ref{eq:loss}) as follows.

\begin{align}
\mathcal{L}_{\theta} &= \log \left( \frac{\sum_{\mathbf{a}\in \mathcal{B}^{-1}(\mathbf{w})} p(\mathbf{a})\exp(f_{\theta}(\mathbf{X},\mathbf{a}))}{\mathbb{E}_{p(\mathbf{a'})}[\exp(f_{\theta}(\mathbf{X},\mathbf{a'}))]} \right) \\
&= \log \sum_{\mathbf{a}\in \mathcal{B}^{-1}(\mathbf{w})} \left( \frac{ \exp(\sum_t \phi_{a_t}^t)}{\sum_{\mathbf{a}^{\prime}} \exp(\sum_t \phi_{a_t^{\prime}}^t)} \right) \\
&= \log \sum_{\mathbf{a}\in \mathcal{B}^{-1}(\mathbf{w})} \left( \frac{\prod_t \exp(\phi_{a_t}^t)}{\sum_{\mathbf{a}^{\prime}} \prod_t \exp(\phi_{a_t^{\prime}}^t)} \right) \\
&= \log \sum_{\mathbf{a}\in \mathcal{B}^{-1}(\mathbf{w})} \left( \frac{\prod_t \exp(\phi_{a_t}^t)}{\prod_t \sum_{\mathbf{a}^{\prime}_t} \exp(\phi_{a_t^{\prime}}^t)} \right) \label{eq:trick} \\
&= \log \sum_{\mathbf{a}\in \mathcal{B}^{-1}(\mathbf{w})} \prod_t \underbrace{\frac{\exp(\phi_{a_t}^t)}{\sum_{\mathbf{a}_t^{\prime}}\exp(\phi_{a_t^{\prime}}^t)}}_{\text{locally normalized scores}} \\
&= \log \sum_{\mathbf{a}\in \mathcal{B}^{-1}(\mathbf{w})} \prod_t p_{\theta}(a_t|\mathbf{X}) \\
&= \log \sum_{\mathbf{a}\in \mathcal{B}^{-1}(\mathbf{w})} P(\mathbf{a}|\mathbf{X}).
\end{align}

\paragraph{} The important step here is the trick in equation~(\ref{eq:trick}), which only holds because of the conditional independence assumption in CTC.

\paragraph{} Therefore, we can compute the CTC objective and its gradients similar to MMI, by locally normalizing $\phi$ (using softmax) and then using the same WFST computations but without the denominator term.